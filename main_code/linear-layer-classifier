import math

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# Define the neural network class
class single_layer_softmax(nn.Module):
    def __init__(self, input_size, output_size):
        super(single_layer_softmax, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
        self.linear.double()

    def forward(self, x):
        return self.linear(x)


# Function to train the model
def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs):
    train_losses = []
    valid_losses = []
    train_accuracies = []
    valid_accuracies = []
    best_valid_accuracy = 0.0  # Track best validation accuracy
    best_model_state = None

    for epoch in range(epochs):
        # Training phase
        model.train()
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            _, predicted = torch.max(outputs.data, 1)
            total_train += targets.size(0)
            correct_train += (predicted == targets).sum().item()
            train_loss += loss.item()

        train_losses.append(train_loss / len(train_loader))
        train_accuracy = correct_train / total_train
        train_accuracies.append(train_accuracy)

        # Validation phase
        model.eval()
        valid_loss = 0.0
        correct_valid = 0
        total_valid = 0
        with torch.no_grad():
            for inputs, targets in valid_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                _, predicted = torch.max(outputs.data, 1)
                total_valid += targets.size(0)
                correct_valid += (predicted == targets).sum().item()
                valid_loss += loss.item()

        valid_losses.append(valid_loss / len(valid_loader))
        valid_accuracy = correct_valid / total_valid
        valid_accuracies.append(valid_accuracy)

        # Check if current model has the best validation accuracy
        if valid_accuracy > best_valid_accuracy:
            best_valid_accuracy = valid_accuracy
            best_model_state = model.state_dict()

        print(f'Epoch [{epoch+1}/{epochs}], '
              f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.4f}, '
              f'Valid Loss: {valid_losses[-1]:.4f}, Valid Acc: {valid_accuracy:.4f}')

    return train_losses, valid_losses, train_accuracies, valid_accuracies, best_model_state


# Main function
def main():
    # Number of classes
    output_size = 4
    # Training set folder path containing vectors
    train_folder_path = '/Users/omerhazan/Desktop/personal/studies/image processing/image_processing_project/PCA/training_vectors_95'
    # Validation set folder path
    valid_folder_path = '/Users/omerhazan/Desktop/personal/studies/image processing/image_processing_project/PCA/testing_vectors_95'

    # Generate training dataset
    train_dataset = []
    for label_folder in os.listdir(train_folder_path):
        label_folder_path = os.path.join(train_folder_path, label_folder)
        if os.path.isdir(label_folder_path):
            label = int(label_folder.split('_')[0])  # Assuming label is the first word in the folder name
            for filename in os.listdir(label_folder_path):
                if filename.endswith(".npy"):
                    vector_path = os.path.join(label_folder_path, filename)
                    vector = np.load(vector_path)
                    train_dataset.append((vector, label))

    # Generate validation dataset
    valid_dataset = []
    for label_folder in os.listdir(valid_folder_path):
        label_folder_path = os.path.join(valid_folder_path, label_folder)
        if os.path.isdir(label_folder_path):
            label = int(label_folder.split('_')[0])  # Assuming label is the first word in the folder name
            for filename in os.listdir(label_folder_path):
                if filename.endswith(".npy"):
                    vector_path = os.path.join(label_folder_path, filename)
                    vector = np.load(vector_path)
                    valid_dataset.append((vector, label))

    # Input size (assuming the size of the vector is consistent across all samples)
    input_size = np.size(vector)

    # Extract inputs and targets from training dataset
    train_inputs, train_targets = zip(*train_dataset)

    # Extract inputs and targets from validation dataset
    valid_inputs, valid_targets = zip(*valid_dataset)

    # Normalize data if required
    normalize_data = input("Do you want to normalize the data? (yes/no): ").lower() == "yes"
    if normalize_data:
        scaler = StandardScaler()
        train_inputs = scaler.fit_transform(train_inputs)
        valid_inputs = scaler.transform(valid_inputs)

    # Create data loaders for training and validation sets
    train_loader = torch.utils.data.DataLoader(list(zip(train_inputs, train_targets)), batch_size=50, shuffle=True)
    valid_loader = torch.utils.data.DataLoader(list(zip(valid_inputs, valid_targets)), batch_size=50, shuffle=False)

    # Define the model
    model = single_layer_softmax(input_size, output_size)

    # Define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=0)  # Changed optimizer to Adam

    # Train the model
    train_losses, valid_losses, train_accuracies, valid_accuracies, best_model_state = train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=400)

    # Plot training and validation loss
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')
    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.grid(True)
    plt.legend()

    # Plot training and validation accuracy
    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Training Accuracy')
    plt.plot(range(1, len(valid_accuracies) + 1), valid_accuracies, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Save the best model
    torch.save(best_model_state, f'/Users/omerhazan/Desktop/personal/studies/image '
                                 f'processing/image_processing_project/trained_model/best_model_hist_eq_{round(max(valid_accuracies),3)}.pth')


if __name__ == "__main__":
    main()
